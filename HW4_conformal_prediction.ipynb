{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Imports, config, and utility setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, math, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "SEED = 123\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "def set_deterministic():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class Timer:\n",
        "    def __init__(self, name=\"block\"):\n",
        "        self.name = name\n",
        "        self.t0 = None\n",
        "    def __enter__(self):\n",
        "        self.t0 = time.time()\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        print(f\"{self.name} took {time.time() - self.t0:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Dataset paths, transforms, and loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "IMAGENET_ROOT = r\"C:\\Users\\ducta\\Downloads\"\n",
        "IMAGENETV2_ROOT = r\"C:\\Users\\ducta\\Downloads\\imagenetv2-matched-frequency\"\n",
        "\n",
        "IMAGENET_VAL_DIR = os.path.join(IMAGENET_ROOT, \"imagenet-val\")\n",
        "IMAGENETV2_DIR = IMAGENETV2_ROOT\n",
        "print(\"ImageNet-Val dir:\", IMAGENET_VAL_DIR)\n",
        "print(\"ImageNetV2 dir:\", IMAGENETV2_DIR)\n",
        "\n",
        "if not os.path.isdir(IMAGENET_VAL_DIR):\n",
        "    raise RuntimeError(\"ImageNet-Val directory not found. Please set IMAGENET_VAL_DIR correctly.\")\n",
        "if not os.path.isdir(IMAGENETV2_DIR):\n",
        "    raise RuntimeError(\"ImageNet-V2 directory not found. Please set IMAGENETV2_DIR correctly.\")\n",
        "\n",
        "imagenet_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "imagenet_val_ds = datasets.ImageFolder(IMAGENET_VAL_DIR, transform=imagenet_transform)\n",
        "print(\"ImageNet-Val size:\", len(imagenet_val_ds))\n",
        "imagenetv2_ds = datasets.ImageFolder(IMAGENETV2_DIR, transform=imagenet_transform)\n",
        "print(\"ImageNet-V2 size:\", len(imagenetv2_ds))\n",
        "\n",
        "def make_loader(dataset, batch_size=64, num_workers=4):\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Base models and temperature scaling (Platt scaling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_model(model_name=\"resnet152\"):\n",
        "    if model_name == \"resnet152\":\n",
        "        model = torchvision.models.resnet152(weights=\"IMAGENET1K_V1\")\n",
        "    else:\n",
        "        model = getattr(torchvision.models, model_name)(weights=\"IMAGENET1K_V1\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_logits(model, dataset, batch_size=64, num_workers=4):\n",
        "    loader = make_loader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
        "    all_logits, all_labels = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        logits = model(x)\n",
        "        all_logits.append(logits.cpu())\n",
        "        all_labels.append(y)\n",
        "    return torch.cat(all_logits, dim=0), torch.cat(all_labels, dim=0)\n",
        "\n",
        "class TemperatureScaler(nn.Module):\n",
        "    def __init__(self, init_T=1.0):\n",
        "        super().__init__()\n",
        "        self.log_T = nn.Parameter(torch.tensor(math.log(init_T)))\n",
        "    @property\n",
        "    def T(self):\n",
        "        return torch.exp(self.log_T)\n",
        "    def forward(self, logits):\n",
        "        return logits / self.T\n",
        "\n",
        "def fit_temperature(logits_cal, labels_cal, lr=0.01, steps=200):\n",
        "    scaler = TemperatureScaler().to(device)\n",
        "    logits_cal = logits_cal.to(device)\n",
        "    labels_cal = labels_cal.to(device)\n",
        "    opt = torch.optim.Adam([scaler.log_T], lr=lr)\n",
        "    for _ in range(steps):\n",
        "        opt.zero_grad()\n",
        "        loss = F.cross_entropy(scaler(logits_cal), labels_cal)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    return scaler.T.detach().item()\n",
        "\n",
        "def apply_temperature(logits, T):\n",
        "    return F.softmax(logits / T, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Helper: sorting probabilities and ranking the true label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def sort_probs_and_get_L(probs, labels):\n",
        "    sorted_probs, sorted_idx = torch.sort(probs, dim=1, descending=True)\n",
        "    sorted_probs_np = sorted_probs.cpu().numpy()\n",
        "    sorted_idx_np = sorted_idx.cpu().numpy()\n",
        "    labels_np = labels.cpu().numpy()\n",
        "    N, K = sorted_probs_np.shape\n",
        "    L = np.empty(N, dtype=np.int64)\n",
        "    for i in range(N):\n",
        "        L[i] = np.where(sorted_idx_np[i] == labels_np[i])[0][0] + 1\n",
        "    return sorted_probs_np, sorted_idx_np, L\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Implement Naive, APS, and RAPS (Algorithms 1\u20133)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def naive_sets(sorted_probs, sorted_idx, alpha):\n",
        "    N, K = sorted_probs.shape\n",
        "    cum = np.cumsum(sorted_probs, axis=1)\n",
        "    threshold = 1.0 - alpha\n",
        "    L = (cum >= threshold).argmax(axis=1) + 1\n",
        "    sets = [sorted_idx[i, :L[i]] for i in range(N)]\n",
        "    return sets, L\n",
        "\n",
        "def conformal_quantile(E, alpha):\n",
        "    n = len(E)\n",
        "    q = math.ceil((n + 1) * (1 - alpha))\n",
        "    E_sorted = np.sort(E)\n",
        "    idx = min(q, n) - 1\n",
        "    return E_sorted[idx]\n",
        "\n",
        "def aps_sets(sorted_probs, sorted_idx, tau):\n",
        "    cum = np.cumsum(sorted_probs, axis=1)\n",
        "    L = (cum <= tau).sum(axis=1)\n",
        "    L = np.clip(L, 1, sorted_probs.shape[1])\n",
        "    sets = [sorted_idx[i, :L[i]] for i in range(len(sorted_probs))]\n",
        "    return sets, L\n",
        "\n",
        "def raps_calibration(sorted_probs, sorted_idx, L_cal, alpha, k_reg=5, lam=0.2):\n",
        "    n, K = sorted_probs.shape\n",
        "    E = np.empty(n, dtype=np.float64)\n",
        "    for i in range(n):\n",
        "        Li = L_cal[i]\n",
        "        base = sorted_probs[i, :Li].sum()\n",
        "        penalty = lam * max(Li - k_reg, 0)\n",
        "        E[i] = base + penalty\n",
        "    tau = conformal_quantile(E, alpha)\n",
        "    return tau\n",
        "\n",
        "def raps_sets(sorted_probs, sorted_idx, tau, k_reg=5, lam=0.2):\n",
        "    N, K = sorted_probs.shape\n",
        "    sets = []\n",
        "    L_out = np.empty(N, dtype=np.int64)\n",
        "    for i in range(N):\n",
        "        cum = 0.0\n",
        "        L = 0\n",
        "        for j in range(K):\n",
        "            rank = j + 1\n",
        "            base = cum + sorted_probs[i, j]\n",
        "            penalty = lam * max(rank - k_reg, 0)\n",
        "            if base + penalty <= tau:\n",
        "                cum = base\n",
        "                L = rank\n",
        "            else:\n",
        "                break\n",
        "        if L == 0:\n",
        "            L = 1\n",
        "        sets.append(sorted_idx[i, :L])\n",
        "        L_out[i] = L\n",
        "    return sets, L_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Metrics: coverage, average set size, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def coverage(pred_sets, labels):\n",
        "    hits = 0\n",
        "    for i, S in enumerate(pred_sets):\n",
        "        if labels[i].item() in S:\n",
        "            hits += 1\n",
        "    return hits / len(labels)\n",
        "\n",
        "def avg_size(pred_sets):\n",
        "    return np.mean([len(S) for S in pred_sets])\n",
        "\n",
        "def topk_accuracies(probs, labels, ks=(1, 5)):\n",
        "    maxk = max(ks)\n",
        "    topk = probs.topk(maxk, dim=1).indices\n",
        "    res = {}\n",
        "    labels = labels.view(-1, 1)\n",
        "    for k in ks:\n",
        "        correct = (topk[:, :k] == labels).any(dim=1).float().mean().item()\n",
        "        res[k] = correct\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Experiment 1: coverage vs set size on ImageNet-Val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "set_deterministic()\n",
        "model = get_model(\"resnet152\")\n",
        "with Timer(\"Collect ImageNet-Val logits\"):\n",
        "    logits_val, labels_val = collect_logits(model, imagenet_val_ds, batch_size=64)\n",
        "alpha_values = [0.1, 0.05]\n",
        "N_TRIALS = 5\n",
        "results_exp1 = []\n",
        "first_split = None\n",
        "for trial in range(N_TRIALS):\n",
        "    perm = torch.randperm(len(logits_val))\n",
        "    cal_idx = perm[:20000]\n",
        "    eval_idx = perm[20000:40000]\n",
        "    logits_cal = logits_val[cal_idx]\n",
        "    labels_cal = labels_val[cal_idx]\n",
        "    logits_eval = logits_val[eval_idx]\n",
        "    labels_eval = labels_val[eval_idx]\n",
        "    T = fit_temperature(logits_cal, labels_cal)\n",
        "    probs_all = F.softmax(logits_val / T, dim=1)\n",
        "    probs_cal = probs_all[cal_idx]\n",
        "    probs_eval = probs_all[eval_idx]\n",
        "    sorted_cal, idx_cal, L_cal = sort_probs_and_get_L(probs_cal, labels_cal)\n",
        "    sorted_eval, idx_eval, L_eval = sort_probs_and_get_L(probs_eval, labels_eval)\n",
        "    topk = topk_accuracies(probs_eval, labels_eval)\n",
        "    for alpha in alpha_values:\n",
        "        naive_sets_eval, naive_L = naive_sets(sorted_eval, idx_eval, alpha)\n",
        "        naive_cov = coverage(naive_sets_eval, labels_eval)\n",
        "        naive_size = avg_size(naive_sets_eval)\n",
        "        tau_aps = raps_calibration(sorted_cal, idx_cal, L_cal, alpha, k_reg=5, lam=0.0)\n",
        "        aps_sets_eval, aps_L = raps_sets(sorted_eval, idx_eval, tau_aps, k_reg=5, lam=0.0)\n",
        "        aps_cov = coverage(aps_sets_eval, labels_eval)\n",
        "        aps_size = avg_size(aps_sets_eval)\n",
        "        tau_raps = raps_calibration(sorted_cal, idx_cal, L_cal, alpha, k_reg=5, lam=0.2)\n",
        "        raps_sets_eval, raps_L = raps_sets(sorted_eval, idx_eval, tau_raps, k_reg=5, lam=0.2)\n",
        "        raps_cov = coverage(raps_sets_eval, labels_eval)\n",
        "        raps_size = avg_size(raps_sets_eval)\n",
        "        results_exp1.append({\"method\": \"Naive\", \"alpha\": alpha, \"trial\": trial, \"coverage\": naive_cov, \"avg_size\": naive_size, \"top1\": topk[1], \"top5\": topk[5]})\n",
        "        results_exp1.append({\"method\": \"APS\", \"alpha\": alpha, \"trial\": trial, \"coverage\": aps_cov, \"avg_size\": aps_size, \"top1\": topk[1], \"top5\": topk[5]})\n",
        "        results_exp1.append({\"method\": \"RAPS\", \"alpha\": alpha, \"trial\": trial, \"coverage\": raps_cov, \"avg_size\": raps_size, \"top1\": topk[1], \"top5\": topk[5]})\n",
        "    if trial == 0:\n",
        "        first_split = {\"cal_idx\": cal_idx, \"eval_idx\": eval_idx, \"probs_all\": probs_all, \"probs_eval\": probs_eval, \"sorted_eval\": sorted_eval, \"idx_eval\": idx_eval, \"labels_eval\": labels_eval, \"labels_cal\": labels_cal, \"sorted_cal\": sorted_cal, \"L_cal\": L_cal, \"L_eval\": L_eval}\n",
        "results_exp1_df = pd.DataFrame(results_exp1)\n",
        "summary_exp1 = results_exp1_df.groupby([\"method\", \"alpha\"]).mean().reset_index()\n",
        "print(summary_exp1)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "for method in [\"Naive\", \"APS\", \"RAPS\"]:\n",
        "    for alpha in alpha_values:\n",
        "        subset = results_exp1_df[(results_exp1_df.method == method) & (results_exp1_df.alpha == alpha)]\n",
        "        axes[0].scatter([1 - alpha], [subset.coverage.mean()], label=f\"{method} \u03b1={alpha}\")\n",
        "        axes[1].scatter([1 - alpha], [subset.avg_size.mean()], label=f\"{method} \u03b1={alpha}\")\n",
        "axes[0].plot([0, 1], [0, 1], \"k--\")\n",
        "axes[0].set_xlabel(\"Target coverage\")\n",
        "axes[0].set_ylabel(\"Empirical coverage\")\n",
        "axes[1].set_xlabel(\"Target coverage\")\n",
        "axes[1].set_ylabel(\"Average set size\")\n",
        "axes[0].legend()\n",
        "axes[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Experiment 2: coverage vs set size on ImageNet-V2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "with Timer(\"Collect ImageNet-V2 logits\"):\n",
        "    logits_v2, labels_v2 = collect_logits(model, imagenetv2_ds, batch_size=64)\n",
        "alpha_values_v2 = [0.1, 0.05]\n",
        "N_TRIALS_V2 = 5\n",
        "results_exp2 = []\n",
        "for trial in range(N_TRIALS_V2):\n",
        "    perm = torch.randperm(len(logits_v2))\n",
        "    cal_idx = perm[:5000]\n",
        "    eval_idx = perm[5000:10000]\n",
        "    logits_cal = logits_v2[cal_idx]\n",
        "    labels_cal = labels_v2[cal_idx]\n",
        "    logits_eval = logits_v2[eval_idx]\n",
        "    labels_eval = labels_v2[eval_idx]\n",
        "    T = fit_temperature(logits_cal, labels_cal)\n",
        "    probs_all = F.softmax(logits_v2 / T, dim=1)\n",
        "    probs_cal = probs_all[cal_idx]\n",
        "    probs_eval = probs_all[eval_idx]\n",
        "    sorted_cal, idx_cal, L_cal = sort_probs_and_get_L(probs_cal, labels_cal)\n",
        "    sorted_eval, idx_eval, L_eval = sort_probs_and_get_L(probs_eval, labels_eval)\n",
        "    topk = topk_accuracies(probs_eval, labels_eval)\n",
        "    for alpha in alpha_values_v2:\n",
        "        naive_sets_eval, naive_L = naive_sets(sorted_eval, idx_eval, alpha)\n",
        "        tau_aps = raps_calibration(sorted_cal, idx_cal, L_cal, alpha, k_reg=5, lam=0.0)\n",
        "        aps_sets_eval, aps_L = raps_sets(sorted_eval, idx_eval, tau_aps, k_reg=5, lam=0.0)\n",
        "        tau_raps = raps_calibration(sorted_cal, idx_cal, L_cal, alpha, k_reg=5, lam=0.2)\n",
        "        raps_sets_eval, raps_L = raps_sets(sorted_eval, idx_eval, tau_raps, k_reg=5, lam=0.2)\n",
        "        results_exp2.append({\"method\": \"Naive\", \"alpha\": alpha, \"trial\": trial, \"coverage\": coverage(naive_sets_eval, labels_eval), \"avg_size\": avg_size(naive_sets_eval), \"top1\": topk[1], \"top5\": topk[5]})\n",
        "        results_exp2.append({\"method\": \"APS\", \"alpha\": alpha, \"trial\": trial, \"coverage\": coverage(aps_sets_eval, labels_eval), \"avg_size\": avg_size(aps_sets_eval), \"top1\": topk[1], \"top5\": topk[5]})\n",
        "        results_exp2.append({\"method\": \"RAPS\", \"alpha\": alpha, \"trial\": trial, \"coverage\": coverage(raps_sets_eval, labels_eval), \"avg_size\": avg_size(raps_sets_eval), \"top1\": topk[1], \"top5\": topk[5]})\n",
        "results_exp2_df = pd.DataFrame(results_exp2)\n",
        "summary_exp2 = results_exp2_df.groupby([\"method\", \"alpha\"]).mean().reset_index()\n",
        "print(summary_exp2)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "for method in [\"Naive\", \"APS\", \"RAPS\"]:\n",
        "    for alpha in alpha_values_v2:\n",
        "        subset = results_exp2_df[(results_exp2_df.method == method) & (results_exp2_df.alpha == alpha)]\n",
        "        axes[0].scatter([1 - alpha], [subset.coverage.mean()], label=f\"{method} \u03b1={alpha}\")\n",
        "        axes[1].scatter([1 - alpha], [subset.avg_size.mean()], label=f\"{method} \u03b1={alpha}\")\n",
        "axes[0].plot([0, 1], [0, 1], \"k--\")\n",
        "axes[0].set_xlabel(\"Target coverage\")\n",
        "axes[0].set_ylabel(\"Empirical coverage\")\n",
        "axes[1].set_xlabel(\"Target coverage\")\n",
        "axes[1].set_ylabel(\"Average set size\")\n",
        "axes[0].legend()\n",
        "axes[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Experiment 3: histograms of set sizes (Naive, APS, RAPS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "alpha_hist = 0.1\n",
        "lam_values = [0.01, 0.1, 1.0]\n",
        "cal_idx = first_split[\"cal_idx\"]\n",
        "eval_idx = first_split[\"eval_idx\"]\n",
        "probs_all = first_split[\"probs_all\"]\n",
        "probs_eval = first_split[\"probs_eval\"]\n",
        "sorted_eval = first_split[\"sorted_eval\"]\n",
        "idx_eval = first_split[\"idx_eval\"]\n",
        "labels_eval = first_split[\"labels_eval\"]\n",
        "sorted_cal = first_split[\"sorted_cal\"]\n",
        "L_cal = first_split[\"L_cal\"]\n",
        "naive_sets_eval, naive_L = naive_sets(sorted_eval, idx_eval, alpha_hist)\n",
        "tau_aps = raps_calibration(sorted_cal, idx_eval, L_cal, alpha_hist, k_reg=5, lam=0.0)\n",
        "aps_sets_eval, aps_L = raps_sets(sorted_eval, idx_eval, tau_aps, k_reg=5, lam=0.0)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "bins = np.arange(1, 51)\n",
        "ax.hist(naive_L, bins=bins, alpha=0.5, label=\"Naive\", log=True)\n",
        "ax.hist(aps_L, bins=bins, alpha=0.5, label=\"APS\", log=True)\n",
        "for lam in lam_values:\n",
        "    tau = raps_calibration(sorted_cal, idx_eval, L_cal, alpha_hist, k_reg=5, lam=lam)\n",
        "    r_sets, r_L = raps_sets(sorted_eval, idx_eval, tau, k_reg=5, lam=lam)\n",
        "    ax.hist(r_L, bins=bins, alpha=0.5, label=f\"RAPS \u03bb={lam}\", log=True)\n",
        "ax.set_xlabel(\"Set size\")\n",
        "ax.set_ylabel(\"Frequency (log)\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Experiment 4: adaptiveness w.r.t. image difficulty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "alpha_adapt = 0.1\n",
        "lam_range = [0.0, 0.001, 0.01, 0.1, 1.0]\n",
        "k_reg = 5\n",
        "sorted_eval = first_split[\"sorted_eval\"]\n",
        "idx_eval = first_split[\"idx_eval\"]\n",
        "labels_eval = first_split[\"labels_eval\"]\n",
        "sorted_cal = first_split[\"sorted_cal\"]\n",
        "L_cal = first_split[\"L_cal\"]\n",
        "rank_true = np.empty(len(labels_eval), dtype=np.int64)\n",
        "for i in range(len(labels_eval)):\n",
        "    rank_true[i] = np.where(idx_eval[i] == labels_eval[i].item())[0][0] + 1\n",
        "bins = [(1, 1), (2, 3), (4, 6), (7, 10), (11, 100), (101, 1000)]\n",
        "bin_labels = [\"1\", \"2-3\", \"4-6\", \"7-10\", \"11-100\", \"101-1000\"]\n",
        "records = []\n",
        "for lam in lam_range:\n",
        "    tau = raps_calibration(sorted_cal, idx_eval, L_cal, alpha_adapt, k_reg=k_reg, lam=lam)\n",
        "    sets, L_out = raps_sets(sorted_eval, idx_eval, tau, k_reg=k_reg, lam=lam)\n",
        "    hits = np.array([labels_eval[i].item() in sets[i] for i in range(len(sets))])\n",
        "    for (a, b), name in zip(bins, bin_labels):\n",
        "        mask = (rank_true >= a) & (rank_true <= b)\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        cov = hits[mask].mean()\n",
        "        size = L_out[mask].mean()\n",
        "        records.append({\"bin\": name, \"lambda\": lam, \"coverage\": cov, \"avg_size\": size})\n",
        "adapt_df = pd.DataFrame(records)\n",
        "print(adapt_df.pivot_table(index=\"bin\", columns=\"lambda\", values=\"avg_size\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Notebook polish and summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"Experiment 1 summary:\")\n",
        "print(summary_exp1)\n",
        "print(\"Experiment 2 summary:\")\n",
        "print(summary_exp2)\n",
        "print(\"Adaptiveness summary:\")\n",
        "print(adapt_df)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}